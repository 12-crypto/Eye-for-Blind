# Eye-for-Blind
• Created a process that explains the content of an image in the form of speech through caption generation.

• Trained the InceptionV3 (CNN-encoder) and RNN-decoder on data-set of over 32,000 images for extracting features from
the image and generating texts.

• Further, these texts are positioned according to their priority with help of attention based mechanism to form a
sentence.
gTTS library was used for conversion of caption to speech.
![image](https://user-images.githubusercontent.com/65901214/193335161-49803393-ad2a-47e5-a89f-6ceac0a9d2e9.png)
