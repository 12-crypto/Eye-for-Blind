# Eye-for-Blind
1. Created a process that explains the content of an image in the form of speech through caption generation.

2. Train the InceptionV3 (CNN-encoder) and RNN-decoder on data-set of over 32,000 images for extracting features from
the image and generating texts.

3. These texts are positioned according to their priority with help of attention based mechanism to form a
sentence.

4. gTTS library was used for conversion of caption to speech.

# Result
![image](https://user-images.githubusercontent.com/65901214/193335161-49803393-ad2a-47e5-a89f-6ceac0a9d2e9.png)
