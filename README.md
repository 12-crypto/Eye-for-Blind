# Eye-for-Blind

Automated <b> <i> image to text-speech </i ></b> generator.

A model for generating textual description of a given image based on the objects and actions in the image


## Dataset
The dataset used is Flicker8k (https://www.kaggle.com/ming666/flicker8k-dataset).

![image](https://user-images.githubusercontent.com/65901214/193335161-49803393-ad2a-47e5-a89f-6ceac0a9d2e9.png)


## Results  
| ![image](https://github.com/JiteshGupta17/Image-Captioning/blob/master/Screenshots/76382310-4998c800-637e-11ea-9e42-04891a26f5b8.png) | 
|:--:| 
| *two dogs are playing with each other in the snow .* |
| ![image](https://github.com/JiteshGupta17/Image-Captioning/blob/master/Screenshots/76382497-f3785480-637e-11ea-986c-1af99a1262c7.png) | 
| *a little boy is sitting on a slide in the playground .* |
| ![image](https://github.com/JiteshGupta17/Image-Captioning/blob/master/Screenshots/76382600-43571b80-637f-11ea-975d-e3481df4595f.png) | 
| *two poodles play with each other in the snow .* |
| ![image](https://github.com/JiteshGupta17/Image-Captioning/blob/master/Screenshots/76382753-bcef0980-637f-11ea-8b2a-2884b30ada54.png) | 
| *football players are tackling a football player carrying a football .* |
| ![image](https://github.com/JiteshGupta17/Image-Captioning/blob/master/Screenshots/76382862-0b040d00-6380-11ea-97d7-ec7b0e4d0ee7.png) | 
| *a snowboarder jumps over a snow covered hill .* |
| ![image](https://github.com/JiteshGupta17/Image-Captioning/blob/master/Screenshots/76382877-17886580-6380-11ea-90ae-ab09b6c74428.png) | 
| *a boy in a blue shirt is doing a trick on his skateboard .* |
| ![image](https://github.com/JiteshGupta17/Image-Captioning/blob/master/Screenshots/76382922-3dae0580-6380-11ea-84b4-047bd574bfc0.png) | 
| *a climber is scaling a rock face whilst attached to a rope .* |



## References
<ul>
  <li> https://towardsdatascience.com/image-captioning-with-keras-teaching-computers-to-describe-pictures-c88a46a311b8 </li>
  <li> https://www.youtube.com/watch?v=RLWuzLLSIgw </li>
</ul>

